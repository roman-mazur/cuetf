package res

import "list"

#google_bigquery_table: {
	@jsonschema(schema="https://json-schema.org/draft/2020-12/schema")
	@jsonschema(id="https://github.com/roman-mazur/cuetf/schema/google_bigquery_table")
	close({
		// Specifies column names to use for data clustering. Up to four
		// top-level columns are allowed, and should be specified in
		// descending priority order.
		clustering?: [...string]

		// The time when this table was created, in milliseconds since the
		// epoch.
		creation_time?: number

		// The dataset ID to create the table in. Changing this forces a
		// new resource to be created.
		dataset_id!: string

		// Whether Terraform will be prevented from destroying the
		// instance. When the field is set to true or unset in Terraform
		// state, a terraform apply or terraform destroy that would
		// delete the table will fail. When the field is set to false,
		// deleting the table is allowed.
		deletion_protection?: bool

		// The field description.
		description?: string

		// All of labels (key/value pairs) present on the resource in GCP,
		// including the labels configured through Terraform, other
		// clients and services.
		effective_labels?: [string]: string

		// A hash of the resource.
		etag?: string

		// The time when this table expires, in milliseconds since the
		// epoch. If not present, the table will persist indefinitely.
		// Expired tables will be deleted and their storage reclaimed.
		expiration_time?: number

		// A descriptive name for the table.
		friendly_name?: string

		// (Output-only) A list of autogenerated schema fields.
		generated_schema_columns?: string

		// Whether Terraform will prevent implicitly added columns in
		// schema from showing diff.
		ignore_auto_generated_schema?: bool

		// Mention which fields in schema are to be ignored
		ignore_schema_changes?: [...string]

		// A mapping of labels to assign to the resource.
		//
		// **Note**: This field is non-authoritative, and will only manage
		// the labels present in your configuration.
		// Please refer to the field 'effective_labels' for all of the
		// labels present on the resource.
		labels?: [string]: string

		// The time when this table was last modified, in milliseconds
		// since the epoch.
		last_modified_time?: number
		id?:                 string

		// The geographic location where the table resides. This value is
		// inherited from the dataset.
		location?: string

		// The maximum staleness of data that could be returned when the
		// table (or stale MV) is queried. Staleness encoded as a string
		// encoding of [SQL IntervalValue
		// type](https://cloud.google.com/bigquery/docs/reference/standard-sql/data-types#interval_type).
		max_staleness?: string

		// The geographic location where the table resides. This value is
		// inherited from the dataset.
		num_bytes?: number
		biglake_configuration?: matchN(1, [#biglake_configuration, list.MaxItems(1) & [...#biglake_configuration]])
		encryption_configuration?: matchN(1, [#encryption_configuration, list.MaxItems(1) & [...#encryption_configuration]])
		external_catalog_table_options?: matchN(1, [#external_catalog_table_options, list.MaxItems(1) & [...#external_catalog_table_options]])
		external_data_configuration?: matchN(1, [#external_data_configuration, list.MaxItems(1) & [...#external_data_configuration]])

		// The number of bytes in the table that are considered "long-term
		// storage".
		num_long_term_bytes?: number
		materialized_view?: matchN(1, [#materialized_view, list.MaxItems(1) & [...#materialized_view]])
		range_partitioning?: matchN(1, [#range_partitioning, list.MaxItems(1) & [...#range_partitioning]])
		schema_foreign_type_info?: matchN(1, [#schema_foreign_type_info, list.MaxItems(1) & [...#schema_foreign_type_info]])
		table_constraints?: matchN(1, [#table_constraints, list.MaxItems(1) & [...#table_constraints]])
		table_replication_info?: matchN(1, [#table_replication_info, list.MaxItems(1) & [...#table_replication_info]])
		time_partitioning?: matchN(1, [#time_partitioning, list.MaxItems(1) & [...#time_partitioning]])

		// The number of rows of data in this table, excluding any data in
		// the streaming buffer.
		num_rows?: number

		// The ID of the project in which the resource belongs.
		project?: string
		view?: matchN(1, [#view, list.MaxItems(1) & [...#view]])

		// If set to true, queries over this table require a partition
		// filter that can be used for partition elimination to be
		// specified.
		require_partition_filter?: bool

		// The tags attached to this table. Tag keys are globally unique.
		// Tag key is expected to be in the namespaced format, for
		// example "123456789012/environment" where 123456789012 is the
		// ID of the parent organization or project resource for this tag
		// key. Tag value is expected to be the short name, for example
		// "Production".
		resource_tags?: [string]: string

		// A JSON schema for the table.
		schema?: string

		// The URI of the created resource.
		self_link?: string

		// A unique ID for the resource. Changing this forces a new
		// resource to be created.
		table_id!: string

		// View sets the optional parameter "view": Specifies the view
		// that determines which table information is returned. By
		// default, basic table information and storage statistics
		// (STORAGE_STATS) are returned. Possible values:
		// TABLE_METADATA_VIEW_UNSPECIFIED, BASIC, STORAGE_STATS, FULL
		table_metadata_view?: string

		// The combination of labels configured directly on the resource
		// and default labels configured on the provider.
		terraform_labels?: [string]: string

		// Describes the table type.
		type?: string
	})

	#biglake_configuration: close({
		// The connection specifying the credentials to be used to read
		// and write to external storage, such as Cloud Storage. The
		// connection_id can have the form
		// "&lt;project\_id&gt;.&lt;location\_id&gt;.&lt;connection\_id&gt;"
		// or
		// "projects/&lt;project\_id&gt;/locations/&lt;location\_id&gt;/connections/&lt;connection\_id&gt;".
		connection_id!: string

		// The file format the data is stored in.
		file_format!: string

		// The fully qualified location prefix of the external folder
		// where table data is stored. The '*' wildcard character is not
		// allowed. The URI should be in the format
		// "gs://bucket/path_to_table/"
		storage_uri!: string

		// The table format the metadata only snapshots are stored in.
		table_format!: string
	})

	#encryption_configuration: close({
		// The self link or full name of a key which should be used to
		// encrypt this table. Note that the default bigquery service
		// account will need to have encrypt/decrypt permissions on this
		// key - you may want to see the
		// google_bigquery_default_service_account datasource and the
		// google_kms_crypto_key_iam_binding resource.
		kms_key_name!: string

		// The self link or full name of the kms key version used to
		// encrypt this table.
		kms_key_version?: string
	})

	#external_catalog_table_options: close({
		storage_descriptor?: matchN(1, [_#defs."/$defs/external_catalog_table_options/$defs/storage_descriptor", list.MaxItems(1) & [..._#defs."/$defs/external_catalog_table_options/$defs/storage_descriptor"]])

		// The connection specifying the credentials to be used to read
		// external storage, such as Azure Blob, Cloud Storage, or S3.
		// The connection is needed to read the open source table from
		// BigQuery Engine. The connection_id can have the form
		// <project_id>.<location_id>.<connection_id> or
		// projects/<project_id>/locations/<location_id>/connections/<connection_id>.
		connection_id?: string

		// A map of key value pairs defining the parameters and properties
		// of the open source table. Corresponds with hive meta store
		// table parameters. Maximum size of 4Mib.
		parameters?: [string]: string
	})

	#external_data_configuration: close({
		// Let BigQuery try to autodetect the schema and format of the
		// table.
		autodetect!: bool
		avro_options?: matchN(1, [_#defs."/$defs/external_data_configuration/$defs/avro_options", list.MaxItems(1) & [..._#defs."/$defs/external_data_configuration/$defs/avro_options"]])

		// The compression type of the data source. Valid values are
		// "NONE" or "GZIP".
		compression?: string

		// The connection specifying the credentials to be used to read
		// external storage, such as Azure Blob, Cloud Storage, or S3.
		// The connectionId can have the form
		// "<project>.<location>.<connection_id>" or
		// "projects/<project>/locations/<location>/connections/<connection_id>".
		connection_id?: string

		// The data types that could be used as a target type when
		// converting decimal values.
		decimal_target_types?: [...string]

		// Specifies how source URIs are interpreted for constructing the
		// file set to load. By default source URIs are expanded against
		// the underlying storage. Other options include specifying
		// manifest files. Only applicable to object storage systems.
		file_set_spec_type?: string

		// Indicates if BigQuery should allow extra values that are not
		// represented in the table schema. If true, the extra values are
		// ignored. If false, records with extra columns are treated as
		// bad records, and if there are too many bad records, an invalid
		// error is returned in the job result. The default value is
		// false.
		ignore_unknown_values?: bool

		// Load option to be used together with sourceFormat
		// newline-delimited JSON to indicate that a variant of JSON is
		// being loaded. To load newline-delimited GeoJSON, specify
		// GEOJSON (and sourceFormat must be set to
		// NEWLINE_DELIMITED_JSON).
		json_extension?: string

		// The maximum number of bad records that BigQuery can ignore when
		// reading data.
		max_bad_records?: number

		// Metadata Cache Mode for the table. Set this to enable caching
		// of metadata from external data source.
		metadata_cache_mode?: string
		bigtable_options?: matchN(1, [_#defs."/$defs/external_data_configuration/$defs/bigtable_options", list.MaxItems(1) & [..._#defs."/$defs/external_data_configuration/$defs/bigtable_options"]])
		csv_options?: matchN(1, [_#defs."/$defs/external_data_configuration/$defs/csv_options", list.MaxItems(1) & [..._#defs."/$defs/external_data_configuration/$defs/csv_options"]])
		google_sheets_options?: matchN(1, [_#defs."/$defs/external_data_configuration/$defs/google_sheets_options", list.MaxItems(1) & [..._#defs."/$defs/external_data_configuration/$defs/google_sheets_options"]])
		hive_partitioning_options?: matchN(1, [_#defs."/$defs/external_data_configuration/$defs/hive_partitioning_options", list.MaxItems(1) & [..._#defs."/$defs/external_data_configuration/$defs/hive_partitioning_options"]])
		json_options?: matchN(1, [_#defs."/$defs/external_data_configuration/$defs/json_options", list.MaxItems(1) & [..._#defs."/$defs/external_data_configuration/$defs/json_options"]])
		parquet_options?: matchN(1, [_#defs."/$defs/external_data_configuration/$defs/parquet_options", list.MaxItems(1) & [..._#defs."/$defs/external_data_configuration/$defs/parquet_options"]])

		// Object Metadata is used to create Object Tables. Object Tables
		// contain a listing of objects (with their metadata) found at
		// the sourceUris. If ObjectMetadata is set, sourceFormat should
		// be omitted.
		object_metadata?: string

		// When creating an external table, the user can provide a
		// reference file with the table schema. This is enabled for the
		// following formats: AVRO, PARQUET, ORC.
		reference_file_schema_uri?: string

		// A JSON schema for the external table. Schema is required for
		// CSV and JSON formats and is disallowed for Google Cloud
		// Bigtable, Cloud Datastore backups, and Avro formats when using
		// external tables.
		schema?: string

		// Please see sourceFormat under ExternalDataConfiguration in
		// Bigquery's public API documentation
		// (https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#externaldataconfiguration)
		// for supported formats. To use "GOOGLE_SHEETS" the scopes must
		// include "googleapis.com/auth/drive.readonly".
		source_format?: string

		// A list of the fully-qualified URIs that point to your data in
		// Google Cloud.
		source_uris!: [...string]
	})

	#materialized_view: close({
		// Allow non incremental materialized view definition. The default
		// value is false.
		allow_non_incremental_definition?: bool

		// Specifies if BigQuery should automatically refresh materialized
		// view when the base table is updated. The default is true.
		enable_refresh?: bool

		// A query whose result is persisted.
		query!: string

		// Specifies maximum frequency at which this materialized view
		// will be refreshed. The default is 1800000.
		refresh_interval_ms?: number
	})

	#range_partitioning: close({
		range!: matchN(1, [_#defs."/$defs/range_partitioning/$defs/range", list.MaxItems(1) & [_, ...] & [..._#defs."/$defs/range_partitioning/$defs/range"]])

		// The field used to determine how to create a range-based
		// partition.
		field!: string
	})

	#schema_foreign_type_info: close({
		// Specifies the system which defines the foreign data type.
		type_system!: string
	})

	#table_constraints: close({
		foreign_keys?: matchN(1, [_#defs."/$defs/table_constraints/$defs/foreign_keys", [..._#defs."/$defs/table_constraints/$defs/foreign_keys"]])
		primary_key?: matchN(1, [_#defs."/$defs/table_constraints/$defs/primary_key", list.MaxItems(1) & [..._#defs."/$defs/table_constraints/$defs/primary_key"]])
	})

	#table_replication_info: close({
		// The interval at which the source materialized view is polled
		// for updates. The default is 300000.
		replication_interval_ms?: number

		// The ID of the source dataset.
		source_dataset_id!: string

		// The ID of the source project.
		source_project_id!: string

		// The ID of the source materialized view.
		source_table_id!: string
	})

	#time_partitioning: close({
		// Number of milliseconds for which to keep the storage for a
		// partition.
		expiration_ms?: number

		// The field used to determine how to create a time-based
		// partition. If time-based partitioning is enabled without this
		// value, the table is partitioned based on the load time.
		field?: string

		// The supported types are DAY, HOUR, MONTH, and YEAR, which will
		// generate one partition per day, hour, month, and year,
		// respectively.
		type!: string
	})

	#view: close({
		// A query that BigQuery executes when the view is referenced.
		query!: string

		// Specifies whether to use BigQuery's legacy SQL for this view.
		// If set to false, the view will use BigQuery's standard SQL
		use_legacy_sql?: bool
	})

	_#defs: "/$defs/external_catalog_table_options/$defs/storage_descriptor": close({
		// Specifies the fully qualified class name of the InputFormat
		// (e.g. "org.apache.hadoop.hive.ql.io.orc.OrcInputFormat"). The
		// maximum length is 128 characters.
		input_format?: string
		serde_info?: matchN(1, [_#defs."/$defs/external_catalog_table_options/$defs/storage_descriptor/$defs/serde_info", list.MaxItems(1) & [..._#defs."/$defs/external_catalog_table_options/$defs/storage_descriptor/$defs/serde_info"]])

		// The physical location of the table (e.g.
		// 'gs://spark-dataproc-data/pangea-data/case_sensitive/' or
		// 'gs://spark-dataproc-data/pangea-data/*'). The maximum length
		// is 2056 bytes.
		location_uri?: string

		// Specifies the fully qualified class name of the OutputFormat
		// (e.g. "org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat"). The
		// maximum length is 128 characters.
		output_format?: string
	})

	_#defs: "/$defs/external_catalog_table_options/$defs/storage_descriptor/$defs/serde_info": close({
		// Name of the SerDe. The maximum length is 256 characters.
		name?: string

		// Key-value pairs that define the initialization parameters for
		// the serialization library. Maximum size 10 Kib.
		parameters?: [string]: string

		// Specifies a fully-qualified class name of the serialization
		// library that is responsible for the translation of data
		// between table representation and the underlying low-level
		// input and output format structures. The maximum length is 256
		// characters.
		serialization_library!: string
	})

	_#defs: "/$defs/external_data_configuration/$defs/avro_options": close({
		// If sourceFormat is set to "AVRO", indicates whether to
		// interpret logical types as the corresponding BigQuery data
		// type (for example, TIMESTAMP), instead of using the raw type
		// (for example, INTEGER).
		use_avro_logical_types!: bool
	})

	_#defs: "/$defs/external_data_configuration/$defs/bigtable_options": close({
		// If field is true, then the column families that are not
		// specified in columnFamilies list are not exposed in the table
		// schema. Otherwise, they are read with BYTES type values. The
		// default value is false.
		ignore_unspecified_column_families?: bool
		column_family?: matchN(1, [_#defs."/$defs/external_data_configuration/$defs/bigtable_options/$defs/column_family", [..._#defs."/$defs/external_data_configuration/$defs/bigtable_options/$defs/column_family"]])

		// If field is true, then each column family will be read as a
		// single JSON column. Otherwise they are read as a repeated cell
		// structure containing timestamp/value tuples. The default value
		// is false.
		output_column_families_as_json?: bool

		// If field is true, then the rowkey column families will be read
		// and converted to string. Otherwise they are read with BYTES
		// type values and users need to manually cast them with CAST if
		// necessary. The default value is false.
		read_rowkey_as_string?: bool
	})

	_#defs: "/$defs/external_data_configuration/$defs/bigtable_options/$defs/column_family": close({
		// The encoding of the values when the type is not STRING.
		// Acceptable encoding values are: TEXT - indicates values are
		// alphanumeric text strings. BINARY - indicates values are
		// encoded using HBase Bytes.toBytes family of functions. This
		// can be overridden for a specific column by listing that column
		// in 'columns' and specifying an encoding for it.
		encoding?: string

		// Identifier of the column family.
		family_id?: string

		// If this is set only the latest version of value are exposed for
		// all columns in this column family. This can be overridden for
		// a specific column by listing that column in 'columns' and
		// specifying a different setting for that column.
		only_read_latest?: bool
		column?: matchN(1, [_#defs."/$defs/external_data_configuration/$defs/bigtable_options/$defs/column_family/$defs/column", [..._#defs."/$defs/external_data_configuration/$defs/bigtable_options/$defs/column_family/$defs/column"]])

		// The type to convert the value in cells of this column family.
		// The values are expected to be encoded using HBase
		// Bytes.toBytes function when using the BINARY encoding value.
		// Following BigQuery types are allowed (case-sensitive):
		// "BYTES", "STRING", "INTEGER", "FLOAT", "BOOLEAN", "JSON".
		// Default type is BYTES. This can be overridden for a specific
		// column by listing that column in 'columns' and specifying a
		// type for it.
		type?: string
	})

	_#defs: "/$defs/external_data_configuration/$defs/bigtable_options/$defs/column_family/$defs/column": close({
		// The encoding of the values when the type is not STRING.
		// Acceptable encoding values are: TEXT - indicates values are
		// alphanumeric text strings. BINARY - indicates values are
		// encoded using HBase Bytes.toBytes family of functions.
		// 'encoding' can also be set at the column family level.
		// However, the setting at this level takes precedence if
		// 'encoding' is set at both levels.
		encoding?: string

		// If the qualifier is not a valid BigQuery field identifier i.e.
		// does not match [a-zA-Z][a-zA-Z0-9_]*, a valid identifier must
		// be provided as the column field name and is used as field name
		// in queries.
		field_name?: string

		// If this is set, only the latest version of value in this column
		// are exposed. 'onlyReadLatest' can also be set at the column
		// family level. However, the setting at this level takes
		// precedence if 'onlyReadLatest' is set at both levels.
		only_read_latest?: bool

		// Qualifier of the column. Columns in the parent column family
		// that has this exact qualifier are exposed as . field. If the
		// qualifier is valid UTF-8 string, it can be specified in the
		// qualifierString field. Otherwise, a base-64 encoded value must
		// be set to qualifierEncoded. The column field name is the same
		// as the column qualifier. However, if the qualifier is not a
		// valid BigQuery field identifier i.e. does not match
		// [a-zA-Z][a-zA-Z0-9_]*, a valid identifier must be provided as
		// fieldName.
		qualifier_encoded?: string

		// Qualifier string.
		qualifier_string?: string

		// The type to convert the value in cells of this column. The
		// values are expected to be encoded using HBase Bytes.toBytes
		// function when using the BINARY encoding value. Following
		// BigQuery types are allowed (case-sensitive): "BYTES",
		// "STRING", "INTEGER", "FLOAT", "BOOLEAN", "JSON", Default type
		// is "BYTES". 'type' can also be set at the column family level.
		// However, the setting at this level takes precedence if 'type'
		// is set at both levels.
		type?: string
	})

	_#defs: "/$defs/external_data_configuration/$defs/csv_options": close({
		// Indicates if BigQuery should accept rows that are missing
		// trailing optional columns.
		allow_jagged_rows?: bool

		// Indicates if BigQuery should allow quoted data sections that
		// contain newline characters in a CSV file. The default value is
		// false.
		allow_quoted_newlines?: bool

		// The character encoding of the data. The supported values are
		// UTF-8 or ISO-8859-1.
		encoding?: string

		// The separator for fields in a CSV file.
		field_delimiter?: string

		// The value that is used to quote data sections in a CSV file. If
		// your data does not contain quoted sections, set the property
		// value to an empty string. If your data contains quoted newline
		// characters, you must also set the allow_quoted_newlines
		// property to true. The API-side default is ", specified in
		// Terraform escaped as \". Due to limitations with Terraform
		// default values, this value is required to be explicitly set.
		quote!: string

		// The number of rows at the top of a CSV file that BigQuery will
		// skip when reading the data.
		skip_leading_rows?: number
	})

	_#defs: "/$defs/external_data_configuration/$defs/google_sheets_options": close({
		// Range of a sheet to query from. Only used when non-empty. At
		// least one of range or skip_leading_rows must be set. Typical
		// format: "sheet_name!top_left_cell_id:bottom_right_cell_id" For
		// example: "sheet1!A1:B20
		range?: string

		// The number of rows at the top of the sheet that BigQuery will
		// skip when reading the data. At least one of range or
		// skip_leading_rows must be set.
		skip_leading_rows?: number
	})

	_#defs: "/$defs/external_data_configuration/$defs/hive_partitioning_options": close({
		// When set, what mode of hive partitioning to use when reading
		// data.
		mode?: string

		// If set to true, queries over this table require a partition
		// filter that can be used for partition elimination to be
		// specified.
		require_partition_filter?: bool

		// When hive partition detection is requested, a common for all
		// source uris must be required. The prefix must end immediately
		// before the partition key encoding begins.
		source_uri_prefix?: string
	})

	_#defs: "/$defs/external_data_configuration/$defs/json_options": close({
		// The character encoding of the data. The supported values are
		// UTF-8, UTF-16BE, UTF-16LE, UTF-32BE, and UTF-32LE. The default
		// value is UTF-8.
		encoding?: string
	})

	_#defs: "/$defs/external_data_configuration/$defs/parquet_options": close({
		// Indicates whether to use schema inference specifically for
		// Parquet LIST logical type.
		enable_list_inference?: bool

		// Indicates whether to infer Parquet ENUM logical type as STRING
		// instead of BYTES by default.
		enum_as_string?: bool
	})

	_#defs: "/$defs/range_partitioning/$defs/range": close({
		// End of the range partitioning, exclusive.
		end!: number

		// The width of each range within the partition.
		interval!: number

		// Start of the range partitioning, inclusive.
		start!: number
	})

	_#defs: "/$defs/table_constraints/$defs/foreign_keys": close({
		column_references!: matchN(1, [_#defs."/$defs/table_constraints/$defs/foreign_keys/$defs/column_references", list.MaxItems(1) & [_, ...] & [..._#defs."/$defs/table_constraints/$defs/foreign_keys/$defs/column_references"]])
		referenced_table!: matchN(1, [_#defs."/$defs/table_constraints/$defs/foreign_keys/$defs/referenced_table", list.MaxItems(1) & [_, ...] & [..._#defs."/$defs/table_constraints/$defs/foreign_keys/$defs/referenced_table"]])

		// Set only if the foreign key constraint is named.
		name?: string
	})

	_#defs: "/$defs/table_constraints/$defs/foreign_keys/$defs/column_references": close({
		// The column in the primary key that are referenced by the
		// referencingColumn.
		referenced_column!: string

		// The column that composes the foreign key.
		referencing_column!: string
	})

	_#defs: "/$defs/table_constraints/$defs/foreign_keys/$defs/referenced_table": close({
		// The ID of the dataset containing this table.
		dataset_id!: string

		// The ID of the project containing this table.
		project_id!: string

		// The ID of the table. The ID must contain only letters (a-z,
		// A-Z), numbers (0-9), or underscores (_). The maximum length is
		// 1,024 characters. Certain operations allow suffixing of the
		// table ID with a partition decorator, such as
		// sample_table$20190123.
		table_id!: string
	})

	_#defs: "/$defs/table_constraints/$defs/primary_key": close({
		// The columns that are composed of the primary key constraint.
		columns!: [...string]
	})
}
